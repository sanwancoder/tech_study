30min

- 1、深拷贝与浅拷贝
    - [Java深拷贝和浅拷贝](https://juejin.im/post/5c988a7ef265da6116246d11)
- 2、java内部类
    - [搞懂Java内部类](https://juejin.im/post/5a903ef96fb9a063435ef0c8)
    - [Java 内部类](http://www.caodahua.cn/detail/17/)
    - [示例代码](https://github.com/sanwancoder/JavaStudy/tree/master/src/com/wyfdc/go/javabase/inner_class)
    - 内部类的好处
        - 1.内部类提供了更好的封装，可以把内部类隐藏在外部类之内，不允许同一个包中的其他类访问该类。

        - 2.内部类的方法可以直接访问外部类的所有数据，包括私有的数据。

        - 3.内部类所实现的功能使用外部类同样可以实现，只是有时使用内部类更方便。

        - 4.每个内部类都能独立的继承一个接口的实现，所以无论外部类是否已经继承了某个(接口的)实现，对于内部类都没有影响。因此     内部类使得Java的多继承机制变得更加完善。
- 3、java集合
    - List
        - ArrayList
        - LinkedList
        - Vector (线程安全)
        - CopyOnWriteArrayList(通过创建底层数组的新副本来实现的) [ArrayList和CopyOnWriteArrayList](https://blog.csdn.net/u011812294/article/details/77600177)
            - 实现了List接口
            - 内部持有一个ReentrantLock lock = new ReentrantLock();
            - 底层是用volatile transient声明的数组 array
            - 读写分离，写时复制出一个新的数组，完成插入、修改或者移除操作后将新数组赋值给array
    - Set
        - HashSet(hash表)
        - TreeSet(二叉树) [Java常用数据结构之Set之TreeSet](https://juejin.im/post/5bfb6d8ff265da610e7fc2da)
            - 毫不意外的继承了抽象类AbstracSet，方便扩展；
            - 实现了一个NavigableSet接口，和NavigableMap接口类似，提供了各种导航方法；
            - 实现了Cloneable接口，可以克隆；
            - 实现了Serializable接口，可以序列化；
        - LinkedHashSet(HashSet+LinkedHashMap)
        - CopyOnWriteArraySet(线程安全)：底层存储结构竟然是CopyOnWriteArrayList，那么我们就可以知道它的名字的由来了，并且知道它支持并发的原理跟CopyOnWriteArrayList是一样的。 [HashSet和CopyOnWriteArraySet](https://www.jianshu.com/p/f55bf8a8520e)
    - Map
        - HashMap (数组+链表(红黑树))
        - TreeMap (可排序)
        - LinkedHashMap (记录插入时顺序)
        - ConcurrentHashMap(线程安全)
        - HashTable (线程安全)
        - ConcurrentSkipListMap(线程安全的有序的哈希表)
     
- 4、加锁方法 synchronized、lock、cas
    - 并发工具类
        - CountDownLatch(countdown/await)
        - CyclicBarrier
        - Samphore(acquire/release)
- 5、有没有开发底层组件或者模块的经验？
- 6、你最拿手或者得意项目？
- 7、排查慢查询经验 
    - 1、当结果集只有一行数据时使用 LIMIT 1；
    - 2、避免 SELECT *，始终指定你需要的列；
    - 3、使用连接（JOIN）来代替子查询； 
    - 4、尽量少数据量的条件放在 where 子句前面；
    - 5、减少数据库访问次数；
    - 6、删除重复记录、
    - 7、连接多个表时使用表别名减少解析时间；
    - 8、用 exists 替换 distinct（提交一个包含一对多表信息的查询）；
    - 9、避免在索引列上使用计算；
    - 10、避免在索引列上使用 IS NULL 和 IS NOT NULL;
    - 11、避免在索引列上使用!=和<>、in、not in
- 8、值传递
    - [为什么说Java中只有值传递](https://blog.csdn.net/bjweimengshu/article/details/79799485)
    - [为什么大家都说java是值传递？](https://www.jianshu.com/p/f1a075af1669)

- 9.kafka消费积压问题
```
如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？

思考
是什么导致了消息积压？是consumer程序bug？是consumer消费的速度落后于消息生产的速度？
积压了多长时间，积压了多少量？
对业务的影响？

解决思路
1. 如果仅仅是consumer消费的速度落后于消息生产的速度的话，可以考虑采用扩容消费者群组的方式。

2. 如果积压比较严重，积压了上百万、上千万的消息。
修复现有consumer的问题，并将其停掉。
重新创建一个容量更大的topic，比如patition是原来的10倍。
编写一个临时consumer程序，消费原来积压的队列。该consumer不做任何耗时的操作，将消息均匀写入新创建的队列里。
将修复好的consumer部署到原来10倍的机器上消费新队列。
消息积压解决后，恢复原有架构。

3. 如果消息已经丢失
由于有的消息队列有过期失效的机制，造成了大量的消息丢失。
这种情况只能将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入mq里面去。 

大量消息在mq里积压了几个小时了还没解决  

　　几千万条数据在MQ里积压了七八个小时，最简单的方法可以让他恢复消费速度，然后等待几个小时消费完毕。 

　　一个消费者一秒是1000条，一秒3个消费者是3000条，一分钟是18万条，1000多万条 ，所以如果你积压了几百万到上千万的数据，即使消费者恢复了，也需要大概1小时的时间才能恢复过来  

　　一般这个时候，只能操作临时紧急扩容了，具体操作步骤和思路如下：  

　　　　先修复consumer的问题，确保其恢复消费速度，然后将现有cnosumer都停掉

　　　　新建一个topic，partition是原来的10倍，临时建立好原先10倍或者20倍的queue数量

　　　　然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue

　　　　接着临时征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的数据

　　　　这种做法相当于是临时将queue资源和consumer资源扩大10倍，以正常的10倍速度来消费数据

　　　　等快速消费完积压数据之后，得恢复原先部署架构，重新用原先的consumer机器来消费消息
```